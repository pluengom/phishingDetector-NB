{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd1ee7b",
   "metadata": {},
   "source": [
    "# Detector de Phishing usando Naive Bayes \n",
    "\n",
    "En esta tarea Ud. debe implementar un detector de phising usando el modelo Naive Bayes. El modelo Naive asume independencia condicional entre todas las variables binarias observadas $\\mathbf{x}$ (tokens o partes de una URL) dada la clase $c$ ('phishing' o 'no phishing'). \n",
    "\n",
    "$p(\\mathbf{x},c) \\propto p(c)p(\\mathbf{x} | c )$\n",
    "\n",
    "donde\n",
    "$p( \\mathbf{x}| c) = \\prod_{i=1}^D p(x_i | c)$\n",
    "\n",
    "\n",
    "Dado un conjunto de tuplas de URLs en formato binario y sus respectivas etiquetas $\\mathcal D=\\{(\\mathbf{x},c)^j\\}_{j=1}^N$, el estimador de maxima verosimilitud para la distribucion de Bernoulli se calcula a partir de la verosimilitud de los datos observados con los de las distribuciones condicionales de clase:\n",
    "\n",
    "$p(\\mathcal D )= \\prod_j \\prod_i( \\theta_{ci})^{c^j} (1-\\theta_{ci} )^{1-c^j}$.\n",
    "\n",
    "Por lo tanto, el estimador de maxima verosimilitud es:\n",
    "\n",
    "$\\theta_{ci}=\\frac{\\textrm{numero de veces donde } x_i=1 \\textrm{ para la clase }c + \\alpha}{\\textrm{numero de ejemplos para la clase }c + 2\\alpha}$\n",
    "\n",
    "Los datos originales de las imagenes contienen regiones donde siempre los valores son cero. Una forma de suavizar la estimacion de probabilidades condicionales es usar suavizado de Laplace introduciendo un parametro $\\alpha$ (https://en.wikipedia.org/wiki/Additive_smoothing). \n",
    "\n",
    "La probabilidad apriori de clase es:\n",
    "\n",
    "$p(c)=\\frac{\\textrm{numero de ejemplos para la clase }c }{\\textrm{numero total de ejemplos}}$\n",
    "\n",
    "Una vez obtenidos los parametros de las distribuciones de Bernoulli condicionales a la clase, podemos hacer inferencia para nuevos datos $\\mathbf{x^\\ast}$ con el modelo.\n",
    "\n",
    "$p(c | \\mathbf{x^\\ast})=\\frac{p(c)p(\\mathbf{x^\\ast} | c )}{p(\\mathbf{x^\\ast})}$\n",
    "\n",
    "C.D. Manning, P. Raghavan and H. Schuetze (2008). Introduction to Information Retrieval. Cambridge University Press, pp. 234-265. https://nlp.stanford.edu/IR-book/html/htmledition/the-bernoulli-model-1.html\n",
    "\n",
    "A. McCallum and K. Nigam (1998). A comparison of event models for naive Bayes text classification. Proc. AAAI/ICML-98 Workshop on Learning for Text Categorization, pp. 41-48.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stupid-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-madison",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('phishing_site_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a300f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89b3412",
   "metadata": {},
   "source": [
    "Primero obtenemos las distribuciones a priori $p(c)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa061de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_prior(df):\n",
    "    N=df.shape[0]   \n",
    "    prob_c=df['Label'].value_counts().values/N\n",
    "    class_names=df['Label'].value_counts().index\n",
    "    return prob_c,class_names\n",
    "\n",
    "prob_c,class_names=get_prior(df)\n",
    "plt.bar(class_names,prob_c)\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Probabilidad apriori')\n",
    "plt.title('Distribucion de clases')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "persistent-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463f19aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "df=shuffle(df, random_state=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capable-sword",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=df.sample(frac=0.8, random_state=200) #random state is a seed value\n",
    "test=df.drop(train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-princeton",
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-eating",
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76724d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_c,class_names=get_prior(train)\n",
    "\n",
    "prob_c,class_names=get_prior(train)\n",
    "plt.bar(class_names,prob_c)\n",
    "plt.xlabel('Clase')\n",
    "plt.ylabel('Probabilidad apriori')\n",
    "plt.title('Distribucion de clases Entrenamiento')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc08d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dc0499",
   "metadata": {},
   "source": [
    "Ahora calculamos las probabilidades condicionales de clase. Transformamos las URLs en una matriz binaria de tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "tokens=set()\n",
    "\n",
    "for i in range(len(train)):\n",
    "    tokens.update(set(re.split(r'\\.|/|\\?|=',train['URL'].iloc[i].lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yellow-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "D=len(tokens)\n",
    "N=train.shape[0]\n",
    "print(\"El numero total de documentos es {0}, el total de tokens es {1}\".format(N,D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-invalid",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "df_good=train[train.Label=='good']\n",
    "\n",
    "token_freq_good=dict({t:0 for t in tokens})\n",
    "\n",
    "for i in range(len(df_good)):\n",
    "    token_list=re.split(r'\\.|/|\\?|=',df_good['URL'].iloc[i].lower())\n",
    "    for t in token_list:\n",
    "        token_freq_good[t]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64dea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(token_freq_good.items(), key=lambda item: item[1],reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bad=train[train.Label=='bad']\n",
    "\n",
    "token_freq_bad=dict({t:0 for t in tokens})\n",
    "\n",
    "for i in range(len(df_bad)):\n",
    "    token_list=re.split(r'\\.|/|\\?|=',df_bad['URL'].iloc[i].lower())\n",
    "    for t in token_list:\n",
    "        token_freq_bad[t]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(token_freq_bad.items(), key=lambda item: item[1],reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-spending",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bad=df_bad.shape[0]\n",
    "n_good=df_good.shape[0]\n",
    "alpha_val=0.1\n",
    "theta_bad={t:(freq+alpha_val)/(n_bad+2*alpha_val) for t,freq in token_freq_bad.items() }\n",
    "theta_good={t:(freq+alpha_val)/(n_good+2*alpha_val) for t,freq in token_freq_good.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b9918e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(theta_bad.items(), key=lambda item: item[1],reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303cfa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(theta_good.items(), key=lambda item: item[1],reverse=True)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6a7931",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bernoulli\n",
    "\n",
    "prob_x_good={t:bernoulli(p) for t,p in theta_good.items()}\n",
    "prob_x_bad={t:bernoulli(p) for t,p in theta_bad.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e26169",
   "metadata": {},
   "source": [
    "# Inferencia en Datos de Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ffbf9e",
   "metadata": {},
   "source": [
    "Ahora, es posible obtener la probabilidad posterior de clase para los datos de test:\n",
    "\n",
    "$p(c | \\mathbf{x^\\ast})=\\frac{p(c)p(\\mathbf{x^\\ast} | c )}{p(\\mathbf{x^\\ast})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c618a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bcd288",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list=re.split(r'\\.|/|\\?|=',test['URL'].iloc[2].lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af672c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "(theta_good['ro']**0)*(1-theta_good['ro'])**(1-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e09eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_x_good['000001992819278372818381291'].pmf(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3b9b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_x_bad['000001992819278372818381291'].pmf(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e88086",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star={t:(1 if t in token_list else 0) for t in tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a795fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_good=np.prod([prob_x_good[t].pmf(x_i) for t,x_i in x_star.items() if t in prob_x_good.keys()])*prob_c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_bad=np.prod([prob_x_bad[t].pmf(x_i) for t,x_i in x_star.items() if t in prob_x_bad.keys()])*prob_c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c6f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_good,p_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1bb850",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_good=p_good/(p_good+p_bad)\n",
    "p_bad=p_bad/(p_good+p_bad)\n",
    "np.argmax([p_good,p_bad])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6e3985",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat=list()\n",
    "for j in range(test.shape[0]):\n",
    "    token_list=re.split(r'\\.|/|\\?|=',test['URL'].iloc[j].lower())\n",
    "    x_star={t:(1 if t in token_list else 0) for t in tokens}\n",
    "    p_good=np.prod([prob_x_good[t].pmf(x_i) for t,x_i in x_star.items() if t in prob_x_good.keys()])*prob_c[0]\n",
    "    p_bad=np.prod([prob_x_bad[t].pmf(x_i) for t,x_i in x_star.items() if t in prob_x_bad.keys()])*prob_c[1]\n",
    "    p_good=p_good/(p_good+p_bad)\n",
    "    p_bad=p_bad/(p_good+p_bad)\n",
    "    y_hat.append(np.argmax([p_good,p_bad]))\n",
    "    if j==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913efee7",
   "metadata": {},
   "source": [
    "# Naive Bayes usando Scikit-Learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e63bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "import re \n",
    "\n",
    "df=pd.read_csv('phishing_site_urls.csv')\n",
    "df=shuffle(df, random_state=200)\n",
    "\n",
    "train=df.sample(frac=0.8, random_state=200) #random state is a seed value\n",
    "test=df.drop(train.index)\n",
    "n_data=train.shape[0]\n",
    "\n",
    "tokens=set()\n",
    "for i in range(len(train)):\n",
    "    tokens.update(set(re.split(r'\\.|/|\\?|=',train['URL'].iloc[i].lower())))\n",
    "    \n",
    "vectorizer = CountVectorizer(vocabulary=tokens,min_df=1./n_data,max_df=1.0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d6712",
   "metadata": {},
   "source": [
    "Ajustamos el vectorizador con una porcion de los datos de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47ed15bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(min_df=2.2754319338668463e-06,\n",
       "                vocabulary={&#x27;&#x27;, &#x27;\\x02$zn¿\\x88&#x27;, &#x27;\\x05\\t¯7\\x8alø&#x27;,\n",
       "                            &#x27;\\x08\\x9d)&amp;¡\\x1fe¸\\x8b\\x1c&#x27;\n",
       "                            &#x27;¢\\x14à6r\\x18d\\xadnvy¨\\x8bð«ñ3â¸%qñ+û\\x93\\x10è\\x85&#x27;\n",
       "                            &#x27;¸\\x03\\x12$¶gz{þ&#x27;,\n",
       "                            &#x27;\\nø\\x88\\n\\x85áö\\x1d¯º\\x9brê7¶\\x15§&#x27;,\n",
       "                            &#x27;\\x0e\\x82âói&lt;ý\\x0b\\x01ú1\\x19ìþqdå¯5\\x89ípç06&#x27;,\n",
       "                            &#x27;\\x10\\x0e0,°n\\x10\\x0e1,q!¹(7\\x1c&#x27;\n",
       "                            &#x27;66²²$¦\\x9c¸2\\x91\\x01t¬!y((\\x93\\x8d\\x87¦ùµ\\x8...\n",
       "                            &#x27;\\x1c&#x27;\n",
       "                            &#x27;\\x88\\x97\\x89ôñ¿\\x8cj²ã\\x87crè\\x147½\\x0eýg\\t&#x27;\n",
       "                            &#x27;&gt;¸4p!ÿ\\x8fz\\x95ôh°ú&lt;af+¹×\\x86\\x04j÷|×¹æ\\x87ô¾£\\x95r\\x85&#x27;\n",
       "                            &#x27;\\x0b&#x27;\n",
       "                            &#x27;\\x8f&#x27;,\n",
       "                            &#x27;\\x1dü\\x8dçàì\\x81ëuvmq&lt;º&#x27;,\n",
       "                            &#x27;\\x1f\\x9eók¦¾þ\\x13)j&amp;´^áyêïfg&gt;|½}¸!\\x11\\x85&#x27;\n",
       "                            &#x27;\\x1e&#x27;\n",
       "                            &#x27;î©q\\x87¶ú5y½êtj\\x90à&amp;ëû\\x1b\\x1e&#x27;\n",
       "                            &quot;n&#x27;\\x1fo;âþ!³\\x08ñ®\\x8d\\x9af·næn\\x05\\x1c&quot;\n",
       "                            &#x27;î\\x93k&#x27;,\n",
       "                            &#x27;\\x1få6åô&#x27;, &#x27; &#x27;, &quot;    url:&#x27;&quot;, &#x27; 1yzphtum&#x27;, &#x27; a&#x27;,\n",
       "                            &#x27; at&#x27;, &#x27; babicz123&#x27;, &#x27; biz&#x27;, &#x27; ch&#x27;, ...})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(min_df=2.2754319338668463e-06,\n",
       "                vocabulary={&#x27;&#x27;, &#x27;\\x02$zn¿\\x88&#x27;, &#x27;\\x05\\t¯7\\x8alø&#x27;,\n",
       "                            &#x27;\\x08\\x9d)&amp;¡\\x1fe¸\\x8b\\x1c&#x27;\n",
       "                            &#x27;¢\\x14à6r\\x18d\\xadnvy¨\\x8bð«ñ3â¸%qñ+û\\x93\\x10è\\x85&#x27;\n",
       "                            &#x27;¸\\x03\\x12$¶gz{þ&#x27;,\n",
       "                            &#x27;\\nø\\x88\\n\\x85áö\\x1d¯º\\x9brê7¶\\x15§&#x27;,\n",
       "                            &#x27;\\x0e\\x82âói&lt;ý\\x0b\\x01ú1\\x19ìþqdå¯5\\x89ípç06&#x27;,\n",
       "                            &#x27;\\x10\\x0e0,°n\\x10\\x0e1,q!¹(7\\x1c&#x27;\n",
       "                            &#x27;66²²$¦\\x9c¸2\\x91\\x01t¬!y((\\x93\\x8d\\x87¦ùµ\\x8...\n",
       "                            &#x27;\\x1c&#x27;\n",
       "                            &#x27;\\x88\\x97\\x89ôñ¿\\x8cj²ã\\x87crè\\x147½\\x0eýg\\t&#x27;\n",
       "                            &#x27;&gt;¸4p!ÿ\\x8fz\\x95ôh°ú&lt;af+¹×\\x86\\x04j÷|×¹æ\\x87ô¾£\\x95r\\x85&#x27;\n",
       "                            &#x27;\\x0b&#x27;\n",
       "                            &#x27;\\x8f&#x27;,\n",
       "                            &#x27;\\x1dü\\x8dçàì\\x81ëuvmq&lt;º&#x27;,\n",
       "                            &#x27;\\x1f\\x9eók¦¾þ\\x13)j&amp;´^áyêïfg&gt;|½}¸!\\x11\\x85&#x27;\n",
       "                            &#x27;\\x1e&#x27;\n",
       "                            &#x27;î©q\\x87¶ú5y½êtj\\x90à&amp;ëû\\x1b\\x1e&#x27;\n",
       "                            &quot;n&#x27;\\x1fo;âþ!³\\x08ñ®\\x8d\\x9af·næn\\x05\\x1c&quot;\n",
       "                            &#x27;î\\x93k&#x27;,\n",
       "                            &#x27;\\x1få6åô&#x27;, &#x27; &#x27;, &quot;    url:&#x27;&quot;, &#x27; 1yzphtum&#x27;, &#x27; a&#x27;,\n",
       "                            &#x27; at&#x27;, &#x27; babicz123&#x27;, &#x27; biz&#x27;, &#x27; ch&#x27;, ...})</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "CountVectorizer(min_df=2.2754319338668463e-06,\n",
       "                vocabulary={'', '\\x02$zn¿\\x88', '\\x05\\t¯7\\x8alø',\n",
       "                            '\\x08\\x9d)&¡\\x1fe¸\\x8b\\x1c'\n",
       "                            '¢\\x14à6r\\x18d\\xadnvy¨\\x8bð«ñ3â¸%qñ+û\\x93\\x10è\\x85'\n",
       "                            '¸\\x03\\x12$¶gz{þ',\n",
       "                            '\\nø\\x88\\n\\x85áö\\x1d¯º\\x9brê7¶\\x15§',\n",
       "                            '\\x0e\\x82âói<ý\\x0b\\x01ú1\\x19ìþqdå¯5\\x89ípç06',\n",
       "                            '\\x10\\x0e0,°n\\x10\\x0e1,q!¹(7\\x1c'\n",
       "                            '66²²$¦\\x9c¸2\\x91\\x01t¬!y((\\x93\\x8d\\x87¦ùµ\\x8...\n",
       "                            '\\x1c'\n",
       "                            '\\x88\\x97\\x89ôñ¿\\x8cj²ã\\x87crè\\x147½\\x0eýg\\t'\n",
       "                            '>¸4p!ÿ\\x8fz\\x95ôh°ú<af+¹×\\x86\\x04j÷|×¹æ\\x87ô¾£\\x95r\\x85'\n",
       "                            '\\x0b'\n",
       "                            '\\x8f',\n",
       "                            '\\x1dü\\x8dçàì\\x81ëuvmq<º',\n",
       "                            '\\x1f\\x9eók¦¾þ\\x13)j&´^áyêïfg>|½}¸!\\x11\\x85'\n",
       "                            '\\x1e'\n",
       "                            'î©q\\x87¶ú5y½êtj\\x90à&ëû\\x1b\\x1e'\n",
       "                            \"n'\\x1fo;âþ!³\\x08ñ®\\x8d\\x9af·næn\\x05\\x1c\"\n",
       "                            'î\\x93k',\n",
       "                            '\\x1få6åô', ' ', \"    url:'\", ' 1yzphtum', ' a',\n",
       "                            ' at', ' babicz123', ' biz', ' ch', ...})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit(train['URL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ac6dd9",
   "metadata": {},
   "source": [
    "Ahora ajustamos el modelo mediante aprendizaje incremental, lo cual nos permite escalar el cómputo cuando no es posible almacenar los datos de entrenamiento en memoria. \n",
    "\n",
    "https://scikit-learn.org/stable/computing/scaling_strategies.html?highlight=out+core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8178ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "clf = BernoulliNB(alpha=0.2)\n",
    "\n",
    "batch_size=5000\n",
    "n_batches=train.shape[0]//batch_size\n",
    "for i in range(n_batches + 1):\n",
    "    mini_batch = train[i*batch_size:(i+1)*batch_size]\n",
    "    X_train = vectorizer.transform(mini_batch['URL'])\n",
    "    y_train = (mini_batch['Label']=='bad').astype('int')\n",
    "    if X_train.shape[0]>0:\n",
    "        clf.partial_fit(X_train, y_train,classes=[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecb094",
   "metadata": {},
   "source": [
    "Una vez que entrenamos el modelo, podemos evaluar en datos de test y comparar con la etiqueta verdadera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcfc2da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=5000\n",
    "n_batches=test.shape[0]//batch_size\n",
    "y_hat=list()\n",
    "y_true=list()\n",
    "for i in range(n_batches + 1):\n",
    "    mini_batch = test[i*batch_size:(i+1)*batch_size]\n",
    "    X_test=vectorizer.transform(mini_batch['URL'])\n",
    "    y_test=(mini_batch['Label']=='bad').astype('int')\n",
    "    if X_test.shape[0]>0:\n",
    "        y_pred=clf.predict(X_test)\n",
    "        y_true.extend(y_test)\n",
    "        y_hat.extend(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "332a5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names=train['Label'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "933a28d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        good       0.97      0.99      0.98     78462\n",
      "         bad       0.97      0.92      0.95     31407\n",
      "\n",
      "    accuracy                           0.97    109869\n",
      "   macro avg       0.97      0.96      0.96    109869\n",
      "weighted avg       0.97      0.97      0.97    109869\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "print(classification_report(y_true, y_hat, target_names=target_names))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "28729dde3b12b26835894d8476d44eb931e70742846a1bd2c46ee3e267788273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
